---
title: "HW2 Data Wrangling I"
author: "Minghui Wang" 
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---

```{r setup, echo = FALSE, message = FALSE}
library(tidyverse)
library(readxl)
library(haven)
```
[HW2](https://p8105.com/homework_2.html) assignment reinforces ideas in [Data Wrangling I](https://p8105.com/topic_data_wrangling_i.html). Here are the codes focusing on question 1-3.

# Problem 1
## Import and clean the NYC Transit.csv
```{r}
trans_entr_df = 
  read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
  col_types = cols( "Route8" = "c","Route9" = "c","Route10" = "c","Route11" = "c")) |>
  janitor::clean_names() |> 
  select(
    line, station_name, station_latitude, station_longitude, starts_with("route"), entry, vending, entrance_type, ada
  ) |> 
  mutate(entry = if_else(entry == "YES", TRUE, FALSE))
```

**A short paragraph about this dataset**:
This dataset contains 32 variables, with 22 being character variables (such as Division, Line, Station Name, and Route1 to Route7), 8 being numeric variables (including Station Latitude, Station Longitude, Entrance Latitude, and Entrance Longitude), and 2 being logical variables (ADA and Free Crossover). So far, I have standardized the column names by converting them to lowercase and treated all route variables as character types. The resulting dataset has dimensions of `r nrow(trans_entr_df)` rows and `r ncol(trans_entr_df)` columns. Currently, the data is not tidy because the route numbers are spread across multiple columns (from "Route1" to "Route11") rather than being combined into a single column as a variable.

## Answer Questions
1. How many distinct stations are there? Note that stations are identified both by name and by line (e.g. 125th St 8th Avenue; 125st Broadway; 125st Lenox); the distinct function may be useful here.
```{r}
trans_entr_df |> 
  select(station_name, line) |>
  distinct()
```
- There are 465 distinct stations.

2. How many stations are ADA compliant?
```{r}
trans_entr_df |> 
  filter(ada == "TRUE")|>
  select(station_name, line) |>
  distinct()
```
- There are 84 distinct stations which are ADA compliant.

3. What proportion of station entrances / exits without vending allow entrance?
```{r}
trans_entr_df |> 
  filter(vending == "NO")|>
  pull(entry) |>
  mean()
```
- 37.70% of station entrances / exits without vending allow entrance.  

## Reformat data 
To make route number and route name distinct variables. 
1. How many distinct stations serve the A train?
```{r, results='hide', message=FALSE}
trans_entr_df |> 
    pivot_longer(
      cols = route1:route11,
      names_to = "route_num", 
      values_to = "route") |> 
    filter(route == "A") |> 
    select(station_name, line) |>
    distinct()
```
- There are 60 distinct stations serve the A train.

2. Of the stations that serve the A train, how many are ADA compliant?
```{r}
trans_entr_df |> 
    pivot_longer(
      cols = route1:route11,
      names_to = "route_num", 
      values_to = "route") |> 
    filter(route == "A", ada == TRUE) |> 
    select(station_name, line) |>
    distinct()
```
- Of the stations that serve the A train, 17 stations are ADA compliant.

# Problem 2

## Read and clean the Mr. Trash Wheel sheet
- specify the sheet in the Excel file and to omit non-data entries (rows with notes / figures; columns containing notes) using arguments in read_excel
- use reasonable variable names
- omit rows that do not include dumpster-specific data
- round the number of sports balls to the nearest integer and converts the result to an integer variable (using as.integer)
```{r}
mr_trash_whl_df =
  read_excel("data/202409_Trash_Wheel_Collection_Data.xlsx" , 
             #specify the sheet in the Excel file
             sheet = 'Mr. Trash Wheel',
             #omit non-data entries
             range = cell_cols(1:14),
             skip = 1,
             na = c("NA", "", "."))|> 
  #use reasonable variable names
  janitor::clean_names()|> 
  #omit rows that do not include dumpster-specific data
  drop_na(dumpster) |>
  #round the number of sports balls to the nearest integer and converts the result to an integer variable
  mutate(sports_balls = as.integer(round(sports_balls)))
```
## Read and clean the Professor Trash Wheel sheet 
```{r}
prof_trash_whl_df = 
  read_excel("data/202409_Trash_Wheel_Collection_Data.xlsx" , 
             #specify the sheet in the Excel file
             sheet = 'Professor Trash Wheel',
             #omit non-data entries
             skip = 1, 
             na = c("NA", "", "."))|> 
  #use reasonable variable names
  janitor::clean_names()|> 
  #omit rows that do not include dumpster-specific data
  drop_na(dumpster) |>
  #round the number of sports balls to the nearest integer and converts the result to an integer variable
  mutate(trash_wheel = "Professor Trash Wheel")
```
## Read and clean the Gwynnda Trash Wheel sheet
```{r}
gwynd_trash_whl_df = 
  read_excel("data/202409_Trash_Wheel_Collection_Data.xlsx" , 
             #specify the sheet in the Excel file
             sheet = 'Gwynnda Trash Wheel',
             #omit non-data entries
             skip = 1, 
             na = c("NA", "", "."))|> 
  #use reasonable variable names
  janitor::clean_names()|> 
  #omit rows that do not include dumpster-specific data
  drop_na(dumpster) |>
  #round the number of sports balls to the nearest integer and converts the result to an integer variable
  mutate(trash_wheel = "Gwynnda Trash Wheel")
```

## Combine these three datasets
- produce a single tidy dataset. 
- add an additional variable to both datasets before combining to keep track of which Trash Wheel is.
```{r}
mr_trash_whl_df = mutate(mr_trash_whl_df, 
                         year = as.double(year),
                         trash_wheel = "Mr. Trash Wheel")

sum_trash_whl_df = 
  bind_rows(mr_trash_whl_df, prof_trash_whl_df, gwynd_trash_whl_df) |> 
  pivot_longer(cols = volume_cubic_yards:sports_balls, 
               names_to = "trash_type", 
               values_to = "amount")|> 
  relocate(trash_wheel) 
```

## Write a paragraph about these data
The combined dataset contains `r nrow(sum_trash_whl_df)` rows, `r nrow(mr_trash_whl_df)+nrow(prof_trash_whl_df)+nrow(gwynd_trash_whl_df)` observations, providing detailed information on trash-related information collected by three trash wheels in Maryland. Variables in the dataset include `r colnames(sum_trash_whl_df)`. Key variables are the trash_type(inncluding original variable "volume_cubic_yards", "cigarette butts", "sports_balls", ect.) and corresponding amount, weight_tons and homes_powered.

The total weight of trash collected by Professor Trash Wheel is `r prof_trash_whl_df |> drop_na(weight_tons) |> pull(weight_tons) |> sum()` tons. Additionally, the total number of cigarette butts collected by Gwynnda in June of 2022 is `r gwynd_trash_whl_df |> filter(year == 2022, month == "June") |> pull(cigarette_butts) |> sum()`.(These calculations are using inline R code, the original code is below).
```{r, message=FALSE}
# The total weight of trash collected by Professor Trash Wheel
prof_trash_whl_df |> 
  drop_na(weight_tons) |> 
  pull(weight_tons) |> 
  sum()
# the total number of cigarette butts collected by Gwynnda in June of 2022
gwynd_trash_whl_df |> 
  filter(year == 2022, month == "June") |> 
  pull(cigarette_butts) |> 
  sum()
```


# Problem 3
## Create a single, well-organized dataset with all the information contained in these data files. 
### Import, clean, tidy, and wrangle of `bakers`
```{r, message = FALSE}
baker_df = read_csv('data/gbb_datasets/bakers.csv',na = c("NA", "", ".")) |>
  janitor::clean_names()|> 
  separate(baker_name, into = c("baker_fist_name", "baker_last_name"), sep = " ")
```
### Import, clean, tidy, and wrangle of `bakes`
```{r, message = FALSE}
bake_df = read_csv('data/gbb_datasets/bakes.csv',
                   na = c("NA", "", ".") )|> 
 janitor::clean_names() |>
 rename(baker_fist_name = baker)
```
### Import, clean, tidy, and wrangle of `results` 
```{r, message = FALSE}
result_df = read_csv('data/gbb_datasets/results.csv',na = c("NA", "", "."),
                     skip = 2) |>
  janitor::clean_names() |>
  rename(baker_fist_name = baker) |>
  mutate(
    result = case_match(
      result, 
      "IN" ~ "stayed in",
      "OUT" ~ "Eliminated",
      "STAR BAKER" ~ "Star Baker",
      "WINNER" ~ "Series Winner",
      "Runner-up" ~ "Series Runner up",
      "WD" ~ "withdrew"
    )
  )
```
### Check for completeness and correctness across datasets
e.g. by viewing individual datasets and using anti_join 

#### 1. `bake_df` and `baker_df`
```{r}
anti_join(bake_df,baker_df, by = "baker_fist_name", "series")
anti_join(baker_df,bake_df, by = "baker_fist_name", "series")
```
I find that "Jo" appearing in the `baker_fist_name column` of the `bake_df` does not appear in the `baker_fist_name column` of `baker_df`. There are 23 bakers informationn apearing in the `baker_df` but the `bake_df` does not includes the information of their bakes. 
This is a lack of completeness and correctness across these two datasets.

#### 2. `bake_df` and `result_df`
```{r}
anti_join(bake_df, result_df, by = "baker_fist_name")
anti_join(result_df, bake_df, by = "baker_fist_name")
```
I find that "Jo" appearing in the `baker_fist_name column` of the `result_df` does not appear in the `baker_fist_name column` of `bake_df` while Joanne appearing in the `baker_fist_name column` of the `result`_df` does not appear in the `baker_fist_name column` of `bake_df`. This is may be an typo error or somethinng wrong, which shows a lack of completeness and correctness across these two datasets.

#### 3. `baker_df` and `result_df`
```{r}
anti_join(baker_df, result_df, by = "baker_fist_name")
anti_join(result_df,baker_df, by = "baker_fist_name")
```
I find that "Jo" appearing in the `baker_fist_name column` of the `result_df` does not appear in the `baker_fist_name column` of `baker_df` while "Joanne" appearing in the `baker_fist_name column` of the `result_df` does not appear in the `baker_fist_name column` of `bake_df`. This is may be an typo error or somethinng wrong, which shows a lack of completeness and correctness across these two datasets.

In a sum, this datasets lacks completeness and correctness across each other.

### Merge to a single, final dataset and organize it
To make variables and observations in meaningful orders. 
```{r}
result_baker_bake_df = result_df |>
  left_join(baker_df, by = c("series", "baker_fist_name")) |>
  left_join(bake_df, by = c("series", "baker_fist_name", "episode")) |>
  relocate(baker_last_name, .after = baker_fist_name)|>
  relocate(technical, .before =show_stopper)
```

###  Export the result file
Save as a CSV in the directory containing the original datasets.
```{r}
write_csv(result_baker_bake_df, "data/result_baker_bake.csv")
```

## Describe data cleaning process
Including any questions you have or choices made. Briefly discuss the final dataset.<br>

1. The data cleaning process involved several key steps. First, all three datasets (`bakers.csv`, `bakes.csv`, and `results.csv`) were imported, specified the missing values and standardized column names to lowercase with underscores, enhancing clarity and consistency. In `baker_df`, the `baker_name` column was split into `baker_fist_name` and `baker_last_name` for better identification of individual bakers and facilitate the future joining with the othe two datasets. Both `bake_df` and `result_df` had the `baker` column renamed to `baker_fist_name` to maintain consisitency across datasets. Lastly, in `result_df`, I convert coded values of the `result` column into more descriptive labels, such as "IN" becoming "stayed in" and "OUT" changing to "Eliminated." 
2. The question for me is "Is it the data clean enought right now? For example, the entry of the `hometown` variable in `bakers.csv` contain both information of town and area/county, sepereated by comma. I have thought to seperate it, but ending with not for I don't think this would influence the final dataset very much and can do it later if necessary.
3. The final datasets records the “Star Baker” of each episodes of each season, with the information of the bake and their bakes of that day. There are 11 variables and 1136 episodes in sum.

## Analyzing star baker trends
Create a reader-friendly table showing the star baker or winner of each episode in Seasons 5 through 10. Comment on this table – were there any predictable overall winners? Any surprises?
### Create a reader-friendly table
```{r}
result_baker_bake_df |> 
  filter(series >= 5 & series <= 10, result == "Star Baker" | result == "Series Winner") |> 
  select(series, episode, baker_fist_name)|> 
  pivot_wider(names_from = series, 
          values_from = baker_fist_name , 
          names_prefix = "baker_star_winner_season_")|> 
knitr::kable()
```
### Conclusion
Based on the total number of wins, the most predictable overall winner would be Richard Burr. Interestingly, despite his 5 wins in Season 5, he didn’t win any further times in Seasons 6 through 10. If we focus on the top performer in Season 10, Steph Blackwell stands out as the overall winner, as all her 4 wins were achieved during that season.

## Viewership data in seasons 1 & 5
Import, clean, tidy, and organize the viewership data in viewers.csv. Show the first 10 rows of this dataset. 
```{r}
viewer_df = read_csv('data/gbb_datasets/viewers.csv', na = c("NA", "", ".")) |> 
  janitor::clean_names() 
head(viewer_df, 10) |> 
knitr::kable()
```
## Average viewership in Season 1 and 5
```{r}
# Because there are null value in viewership in Season 1, we should only calculate the mean among those non-null values, which is row 1-6:
viewer_df |> 
  slice(1:6)|> 
  pull(series_1) |>
  mean()

# Viewership in Season 5
viewer_df |> 
  pull(series_5) |>
  mean()
```
The average viewership in Season 1 is `r viewer_df |> slice(1:6)|> pull(series_1) |> mean()`.
The average viewership in Season 5 is `r viewer_df |> pull(series_5) |> mean()`.