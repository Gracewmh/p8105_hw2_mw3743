HW2 Data Wrangling I
================
Minghui Wang
2024-10-02

[HW2](https://p8105.com/homework_2.html) assignment reinforces ideas in
[Data Wrangling I](https://p8105.com/topic_data_wrangling_i.html). Here
are the codes focusing on question 1-3.

# Problem 1

## Import and clean the NYC Transit.csv

``` r
trans_entr_df = 
  read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
  col_types = cols( "Route8" = "c","Route9" = "c","Route10" = "c","Route11" = "c")) |>
  janitor::clean_names() |> 
  select(
    line, station_name, station_latitude, station_longitude, starts_with("route"), entry, vending, entrance_type, ada
  ) |> 
  mutate(entry = if_else(entry == "YES", TRUE, FALSE))
```

**A short paragraph about this dataset**: This dataset contains 32
variables, with 22 being character variables (such as Division, Line,
Station Name, and Route1 to Route7), 8 being numeric variables
(including Station Latitude, Station Longitude, Entrance Latitude, and
Entrance Longitude), and 2 being logical variables (ADA and Free
Crossover). So far, I have standardized the column names by converting
them to lowercase and treated all route variables as character types.
The resulting dataset has dimensions of 1868 rows and 19 columns.
Currently, the data is not tidy because the route numbers are spread
across multiple columns (from “Route1” to “Route11”) rather than being
combined into a single column as a variable.

## Answer Questions

1.  How many distinct stations are there? Note that stations are
    identified both by name and by line (e.g. 125th St 8th Avenue; 125st
    Broadway; 125st Lenox); the distinct function may be useful here.

``` r
trans_entr_df |> 
  select(station_name, line) |>
  distinct()
```

    ## # A tibble: 465 × 2
    ##    station_name             line    
    ##    <chr>                    <chr>   
    ##  1 25th St                  4 Avenue
    ##  2 36th St                  4 Avenue
    ##  3 45th St                  4 Avenue
    ##  4 53rd St                  4 Avenue
    ##  5 59th St                  4 Avenue
    ##  6 77th St                  4 Avenue
    ##  7 86th St                  4 Avenue
    ##  8 95th St                  4 Avenue
    ##  9 9th St                   4 Avenue
    ## 10 Atlantic Av-Barclays Ctr 4 Avenue
    ## # ℹ 455 more rows

- There are 465 distinct stations.

2.  How many stations are ADA compliant?

``` r
trans_entr_df |> 
  filter(ada == "TRUE")|>
  select(station_name, line) |>
  distinct()
```

    ## # A tibble: 84 × 2
    ##    station_name                   line           
    ##    <chr>                          <chr>          
    ##  1 Atlantic Av-Barclays Ctr       4 Avenue       
    ##  2 DeKalb Av                      4 Avenue       
    ##  3 Pacific St                     4 Avenue       
    ##  4 Grand Central                  42nd St Shuttle
    ##  5 34th St                        6 Avenue       
    ##  6 47-50th Sts Rockefeller Center 6 Avenue       
    ##  7 Church Av                      6 Avenue       
    ##  8 21st St                        63rd Street    
    ##  9 Lexington Av                   63rd Street    
    ## 10 Roosevelt Island               63rd Street    
    ## # ℹ 74 more rows

- There are 84 distinct stations which are ADA compliant.

3.  What proportion of station entrances / exits without vending allow
    entrance?

``` r
trans_entr_df |> 
  filter(vending == "NO")|>
  pull(entry) |>
  mean()
```

    ## [1] 0.3770492

- 37.70% of station entrances / exits without vending allow entrance.

## Reformat data

To make route number and route name distinct variables. 1. How many
distinct stations serve the A train?

``` r
trans_entr_df |> 
    pivot_longer(
      cols = route1:route11,
      names_to = "route_num", 
      values_to = "route") |> 
    filter(route == "A") |> 
    select(station_name, line) |>
    distinct()
```

- There are 60 distinct stations serve the A train.

2.  Of the stations that serve the A train, how many are ADA compliant?

``` r
trans_entr_df |> 
    pivot_longer(
      cols = route1:route11,
      names_to = "route_num", 
      values_to = "route") |> 
    filter(route == "A", ada == TRUE) |> 
    select(station_name, line) |>
    distinct()
```

    ## # A tibble: 17 × 2
    ##    station_name                  line            
    ##    <chr>                         <chr>           
    ##  1 14th St                       8 Avenue        
    ##  2 168th St - Washington Heights 8 Avenue        
    ##  3 175th St                      8 Avenue        
    ##  4 34th St                       8 Avenue        
    ##  5 42nd St                       8 Avenue        
    ##  6 59th St                       8 Avenue        
    ##  7 Inwood - 207th St             8 Avenue        
    ##  8 West 4th St                   8 Avenue        
    ##  9 World Trade Center            8 Avenue        
    ## 10 Times Square-42nd St          Broadway        
    ## 11 59th St-Columbus Circle       Broadway-7th Ave
    ## 12 Times Square                  Broadway-7th Ave
    ## 13 8th Av                        Canarsie        
    ## 14 Franklin Av                   Franklin        
    ## 15 Euclid Av                     Fulton          
    ## 16 Franklin Av                   Fulton          
    ## 17 Howard Beach                  Rockaway

- Of the stations that serve the A train, 17 stations are ADA compliant.

# Problem 2

## Read and clean the Mr. Trash Wheel sheet

- specify the sheet in the Excel file and to omit non-data entries (rows
  with notes / figures; columns containing notes) using arguments in
  read_excel
- use reasonable variable names
- omit rows that do not include dumpster-specific data
- round the number of sports balls to the nearest integer and converts
  the result to an integer variable (using as.integer)

``` r
mr_trash_whl_df =
  read_excel("data/202409_Trash_Wheel_Collection_Data.xlsx" , 
             #specify the sheet in the Excel file
             sheet = 'Mr. Trash Wheel',
             #omit non-data entries
             range = cell_cols(1:14),
             skip = 1,
             na = c("NA", "", "."))|> 
  #use reasonable variable names
  janitor::clean_names()|> 
  #omit rows that do not include dumpster-specific data
  drop_na(dumpster) |>
  #round the number of sports balls to the nearest integer and converts the result to an integer variable
  mutate(sports_balls = as.integer(round(sports_balls)))
```

## Read and clean the Professor Trash Wheel sheet

``` r
prof_trash_whl_df = 
  read_excel("data/202409_Trash_Wheel_Collection_Data.xlsx" , 
             #specify the sheet in the Excel file
             sheet = 'Professor Trash Wheel',
             #omit non-data entries
             skip = 1, 
             na = c("NA", "", "."))|> 
  #use reasonable variable names
  janitor::clean_names()|> 
  #omit rows that do not include dumpster-specific data
  drop_na(dumpster) |>
  #round the number of sports balls to the nearest integer and converts the result to an integer variable
  mutate(trash_wheel = "Professor Trash Wheel")
```

## Read and clean the Gwynnda Trash Wheel sheet

``` r
gwynd_trash_whl_df = 
  read_excel("data/202409_Trash_Wheel_Collection_Data.xlsx" , 
             #specify the sheet in the Excel file
             sheet = 'Gwynnda Trash Wheel',
             #omit non-data entries
             skip = 1, 
             na = c("NA", "", "."))|> 
  #use reasonable variable names
  janitor::clean_names()|> 
  #omit rows that do not include dumpster-specific data
  drop_na(dumpster) |>
  #round the number of sports balls to the nearest integer and converts the result to an integer variable
  mutate(trash_wheel = "Gwynnda Trash Wheel")
```

## Combine these three datasets

- produce a single tidy dataset.
- add an additional variable to both datasets before combining to keep
  track of which Trash Wheel is.

``` r
mr_trash_whl_df = mutate(mr_trash_whl_df, 
                         year = as.double(year),
                         trash_wheel = "Mr. Trash Wheel")

sum_trash_whl_df = 
  bind_rows(mr_trash_whl_df, prof_trash_whl_df, gwynd_trash_whl_df) |> 
  pivot_longer(cols = volume_cubic_yards:sports_balls, 
               names_to = "trash_type", 
               values_to = "amount")|> 
  relocate(trash_wheel) 
```

## Write a paragraph about these data

The combined dataset contains 8264 observations, providing detailed
information on trash-related information collected by three trash wheels
inn Maryland. Key variables in the dataset include trash_wheel,
dumpster, month, year, date, weight_tons, homes_powered, trash_type,
amount. The total weight of trash collected by Professor Trash Wheel is
246.74 tons. Additionally, the total number of cigarette butts collected
by Gwynnda in June of 2022 is 1.812^{4}.

# Problem 3

## Create a single, well-organized dataset with all the information contained in these data files.

### Import, clean, tidy, and wrangle of `bakers`;

``` r
baker_df = read_csv('data/gbb_datasets/bakers.csv',na = c("NA", "", ".")) |>
  janitor::clean_names()|> 
  separate(baker_name, into = c("baker_fist_name", "baker_last_name"), sep = " ")
```

### Import, clean, tidy, and wrangle of \``bakes`;

``` r
bake_df = read_csv('data/gbb_datasets/bakes.csv',
                   na = c("NA", "", ".") )|> 
 janitor::clean_names() |>
 rename(baker_fist_name = baker)
```

### Import, clean, tidy, and wrangle of `results`;

``` r
result_df = read_csv('data/gbb_datasets/results.csv',na = c("NA", "", "."),
                     skip = 2) |>
  janitor::clean_names() |>
  rename(baker_fist_name = baker) |>
  mutate(
    result = case_match(
      result, 
      "IN" ~ "stayed in",
      "OUT" ~ "Eliminated",
      "STAR BAKER" ~ "Star Baker",
      "WINNER" ~ "Series Winner",
      "Runner-up" ~ "Series Runner up",
      "WD" ~ "withdrew"
    )
  )
```

### Check for completeness and correctness across datasets

e.g. by viewing individual datasets and using anti_join

``` r
anti_join(baker_df, bake_df)
```

    ## Joining with `by = join_by(baker_fist_name, series)`

    ## # A tibble: 26 × 6
    ##    baker_fist_name baker_last_name series baker_age baker_occupation    hometown
    ##    <chr>           <chr>            <dbl>     <dbl> <chr>               <chr>   
    ##  1 Alice           Fevronia            10        28 Geography teacher   Essex   
    ##  2 Amelia          LeBruin             10        24 Fashion designer    Halifax 
    ##  3 Antony          Amourdoux            9        30 Banker              London  
    ##  4 Briony          Williams             9        33 Full-time parent    Bristol 
    ##  5 Dan             Beasley-Harling      9        36 Full-time parent    London  
    ##  6 Dan             Chambers            10        32 Support worker      Rotherh…
    ##  7 David           Atherton            10        36 International heal… Whitby  
    ##  8 Helena          Garcia              10        40 Online project man… Leeds   
    ##  9 Henry           Bird                10        20 Student             Durham  
    ## 10 Imelda          McCarron             9        33 Countryside recrea… County …
    ## # ℹ 16 more rows

``` r
anti_join(result_df, baker_df)
```

    ## Joining with `by = join_by(series, baker_fist_name)`

    ## # A tibble: 8 × 5
    ##   series episode baker_fist_name technical result       
    ##    <dbl>   <dbl> <chr>               <dbl> <chr>        
    ## 1      2       1 Joanne                 11 stayed in    
    ## 2      2       2 Joanne                 10 stayed in    
    ## 3      2       3 Joanne                  1 stayed in    
    ## 4      2       4 Joanne                  8 stayed in    
    ## 5      2       5 Joanne                  6 stayed in    
    ## 6      2       6 Joanne                  1 Star Baker   
    ## 7      2       7 Joanne                  3 stayed in    
    ## 8      2       8 Joanne                  1 Series Winner

``` r
anti_join(result_df, bake_df)
```

    ## Joining with `by = join_by(series, episode, baker_fist_name)`

    ## # A tibble: 596 × 5
    ##    series episode baker_fist_name technical result
    ##     <dbl>   <dbl> <chr>               <dbl> <chr> 
    ##  1      1       2 Lea                    NA <NA>  
    ##  2      1       2 Mark                   NA <NA>  
    ##  3      1       3 Annetha                NA <NA>  
    ##  4      1       3 Lea                    NA <NA>  
    ##  5      1       3 Louise                 NA <NA>  
    ##  6      1       3 Mark                   NA <NA>  
    ##  7      1       4 Annetha                NA <NA>  
    ##  8      1       4 Jonathan               NA <NA>  
    ##  9      1       4 Lea                    NA <NA>  
    ## 10      1       4 Louise                 NA <NA>  
    ## # ℹ 586 more rows

### Merge to a single, final dataset and organize it

To make variables and observations in meaningful orders.

``` r
result_baker_bake_df = result_df |>
  left_join(baker_df, by = c("series", "baker_fist_name")) |>
  left_join(bake_df, by = c("series", "baker_fist_name", "episode")) |>
  relocate(baker_last_name, .after = baker_fist_name)|>
  relocate(technical, .before =show_stopper)
view(result_baker_bake_df)
```

### Export the result file

Save as a CSV in the directory containing the original datasets.

``` r
write_csv(result_baker_bake_df, "data/result_baker_bake.csv")
```

## Describe data cleaning process

Including any questions you have or choices made. Briefly discuss the
final dataset.<br>

1.  The data cleaning process involved several key steps. First, all
    three datasets (`bakers.csv`, `bakes.csv`, and `results.csv`) were
    imported, specified the missing values and standardized column names
    to lowercase with underscores, enhancing clarity and consistency. In
    `baker_df`, the `baker_name` column was split into `baker_fist_name`
    and `baker_last_name` for better identification of individual bakers
    and facilitate the future joining with the othe two datasets. Both
    `bake_df` and `result_df` had the `baker` column renamed to
    `baker_fist_name` to maintain consisitency across datasets. Lastly,
    in `result_df`, I convert coded values of the `result` column into
    more descriptive labels, such as “IN” becoming “stayed in” and “OUT”
    changing to “Eliminated.”
2.  The question for me is “Is it the data clean enought right now? For
    example, the entry of the `hometown` variable in `bakers.csv`
    contain both information of town and area/county, sepereated by
    comma. I have thought to seperate it, but ending with not for I
    don’t think this would influence the final dataset very much and can
    do it later if necessary.
3.  The final datasets records the “Star Baker” of each episodes of each
    season, with the information of the bake and their bakes of that
    day. There are 11 variables and 1136 episodes in sum.

## Analyzing star baker trends

Create a reader-friendly table showing the star baker or winner of each
episode in Seasons 5 through 10. Comment on this table – were there any
predictable overall winners? Any surprises?

``` r
result_baker_bake_df |> 
  filter(series >= 5 & series <= 10 & result == "Star Baker") |>
  mutate(
    winner_name = paste(baker_fist_name, baker_last_name),
    series_episode = paste(series, episode, sep = ":"),
    won = 1
  ) |>
  select(series_episode, winner_name, won) |> 
  pivot_wider(
    names_from = series_episode, 
    values_from = won,
    values_fill = list(won = 0)
  ) |> 
  mutate(total_wins = rowSums(across(where(is.numeric)))) |>
  knitr::kable()
```

| winner_name          | 5:1 | 5:2 | 5:3 | 5:4 | 5:5 | 5:6 | 5:7 | 5:8 | 5:9 | 6:1 | 6:2 | 6:3 | 6:4 | 6:5 | 6:6 | 6:7 | 6:8 | 6:9 | 7:1 | 7:2 | 7:3 | 7:4 | 7:5 | 7:6 | 7:7 | 7:8 | 7:9 | 8:1 | 8:2 | 8:3 | 8:4 | 8:5 | 8:6 | 8:7 | 8:8 | 8:9 | 9:1 | 9:2 | 9:3 | 9:4 | 9:5 | 9:6 | 9:7 | 9:8 | 9:9 | 10:1 | 10:2 | 10:3 | 10:4 | 10:5 | 10:6 | 10:7 | 10:8 | 10:9 | total_wins |
|:---------------------|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----------:|
| Nancy Birtwhistle    |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          1 |
| Richard Burr         |   0 |   1 |   0 |   1 |   0 |   0 |   1 |   1 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          5 |
| Luis Troyano         |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          1 |
| Kate Henry           |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          1 |
| Chetna Makan         |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          1 |
| Marie Campbell       |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          1 |
| Ian Cumming          |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   1 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          3 |
| Nadiya Hussain       |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   1 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          3 |
| Mat Riley            |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          1 |
| Tamal Ray            |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          1 |
| Jane Beedle          |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          1 |
| Candice Brown        |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   1 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          3 |
| Tom Gilliford        |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          2 |
| Benjamina Ebuehi     |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          1 |
| Andrew Smyth         |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          2 |
| Steven Carter-Bailey |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   1 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          3 |
| Julia Chernogorova   |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          1 |
| Kate Lyon            |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          1 |
| Sophie Faldo         |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          2 |
| Liam Charles         |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          1 |
| Stacey Hart          |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          1 |
| Manon Lagrave        |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          1 |
| Rahul Mandal         |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          2 |
| Dan Beasley-Harling  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          1 |
| Kim-Joy Hewlett      |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   1 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          2 |
| Briony Williams      |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          1 |
| Ruby Bhogal          |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   1 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          2 |
| Michelle Evans-Fecci |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    1 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |          1 |
| Alice Fevronia       |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    1 |    0 |    0 |    0 |    0 |    0 |    0 |    1 |          2 |
| Michael Chakraverty  |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    1 |    0 |    0 |    0 |    0 |    0 |    0 |          1 |
| Steph Blackwell      |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    1 |    1 |    1 |    0 |    1 |    0 |          4 |
| Henry Bird           |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |    0 |    1 |    0 |    0 |          1 |

Based on the total number of wins, the most predictable overall winner
would be Richard Burr. Interestingly, despite his 5 wins in Season 5, he
didn’t win any further times in Seasons 6 through 10. If we focus on the
top performer in Season 10, Steph Blackwell stands out as the overall
winner, as all her 4 wins were achieved during that season.

## Viewership data in seasons 1 & 5

Import, clean, tidy, and organize the viewership data in viewers.csv.
Show the first 10 rows of this dataset.

``` r
viewer_df = read_csv('data/gbb_datasets/viewers.csv' ,na = c("NA", "", ".")) |>
  janitor::clean_names() 
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
head(viewer_df, 10)
```

    ## # A tibble: 10 × 11
    ##    episode series_1 series_2 series_3 series_4 series_5 series_6 series_7
    ##      <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>
    ##  1       1     2.24     3.1      3.85     6.6      8.51     11.6     13.6
    ##  2       2     3        3.53     4.6      6.65     8.79     11.6     13.4
    ##  3       3     3        3.82     4.53     7.17     9.28     12.0     13.0
    ##  4       4     2.6      3.6      4.71     6.82    10.2      12.4     13.3
    ##  5       5     3.03     3.83     4.61     6.95     9.95     12.4     13.1
    ##  6       6     2.75     4.25     4.82     7.32    10.1      12       13.1
    ##  7       7    NA        4.42     5.1      7.76    10.3      12.4     13.4
    ##  8       8    NA        5.06     5.35     7.41     9.02     11.1     13.3
    ##  9       9    NA       NA        5.7      7.41    10.7      12.6     13.4
    ## 10      10    NA       NA        6.74     9.45    13.5      15.0     15.9
    ## # ℹ 3 more variables: series_8 <dbl>, series_9 <dbl>, series_10 <dbl>

## Average viewership in Season 1 and 5

``` r
# Because there are null value in viewership in Season 1, we should only calculate the mean among those non-null values, which is row 1-6:
viewer_df |> 
  slice(1:6)|> 
  pull(series_1) |>
  mean()
```

    ## [1] 2.77

``` r
# Viewership in Season 5
viewer_df |> 
  pull(series_5) |>
  mean()
```

    ## [1] 10.0393

The average viewership in Season 1 is 2.77. The average viewership in
Season 5 is 10.0393.
