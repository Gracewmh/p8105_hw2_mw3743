HW2 Data Wrangling I
================
Minghui Wang
2024-10-02

[HW2](https://p8105.com/homework_2.html) assignment reinforces ideas in
[Data Wrangling I](https://p8105.com/topic_data_wrangling_i.html). Here
are the codes focusing on question 1-3.

# Problem 1

## Import and clean the NYC Transit.csv

``` r
trans_entr_df = 
  read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
  col_types = cols( "Route8" = "c","Route9" = "c","Route10" = "c","Route11" = "c")) |>
  janitor::clean_names() |> 
  select(
    line, station_name, station_latitude, station_longitude, starts_with("route"), entry, vending, entrance_type, ada
  ) |> 
  mutate(entry = if_else(entry == "YES", TRUE, FALSE))
```

**A short paragraph about this dataset**: This dataset contains 32
variables, with 22 being character variables (such as Division, Line,
Station Name, and Route1 to Route7), 8 being numeric variables
(including Station Latitude, Station Longitude, Entrance Latitude, and
Entrance Longitude), and 2 being logical variables (ADA and Free
Crossover). So far, I have standardized the column names by converting
them to lowercase and treated all route variables as character types.
The resulting dataset has dimensions of 1868 rows and 19 columns.
Currently, the data is not tidy because the route numbers are spread
across multiple columns (from “Route1” to “Route11”) rather than being
combined into a single column as a variable.

## Answer Questions

1.  How many distinct stations are there? Note that stations are
    identified both by name and by line (e.g. 125th St 8th Avenue; 125st
    Broadway; 125st Lenox); the distinct function may be useful here.

``` r
trans_entr_df |> 
  select(station_name, line) |>
  distinct()
```

    ## # A tibble: 465 × 2
    ##    station_name             line    
    ##    <chr>                    <chr>   
    ##  1 25th St                  4 Avenue
    ##  2 36th St                  4 Avenue
    ##  3 45th St                  4 Avenue
    ##  4 53rd St                  4 Avenue
    ##  5 59th St                  4 Avenue
    ##  6 77th St                  4 Avenue
    ##  7 86th St                  4 Avenue
    ##  8 95th St                  4 Avenue
    ##  9 9th St                   4 Avenue
    ## 10 Atlantic Av-Barclays Ctr 4 Avenue
    ## # ℹ 455 more rows

- There are 465 distinct stations.

2.  How many stations are ADA compliant?

``` r
trans_entr_df |> 
  filter(ada == "TRUE")|>
  select(station_name, line) |>
  distinct()
```

    ## # A tibble: 84 × 2
    ##    station_name                   line           
    ##    <chr>                          <chr>          
    ##  1 Atlantic Av-Barclays Ctr       4 Avenue       
    ##  2 DeKalb Av                      4 Avenue       
    ##  3 Pacific St                     4 Avenue       
    ##  4 Grand Central                  42nd St Shuttle
    ##  5 34th St                        6 Avenue       
    ##  6 47-50th Sts Rockefeller Center 6 Avenue       
    ##  7 Church Av                      6 Avenue       
    ##  8 21st St                        63rd Street    
    ##  9 Lexington Av                   63rd Street    
    ## 10 Roosevelt Island               63rd Street    
    ## # ℹ 74 more rows

- There are 84 distinct stations which are ADA compliant.

3.  What proportion of station entrances / exits without vending allow
    entrance?

``` r
trans_entr_df |> 
  filter(vending == "NO")|>
  pull(entry) |>
  mean()
```

    ## [1] 0.3770492

- 37.70% of station entrances / exits without vending allow entrance.

## Reformat data

To make route number and route name distinct variables. 1. How many
distinct stations serve the A train?

``` r
trans_entr_df |> 
    pivot_longer(
      cols = route1:route11,
      names_to = "route_num", 
      values_to = "route") |> 
    filter(route == "A") |> 
    select(station_name, line) |>
    distinct()
```

- There are 60 distinct stations serve the A train.

2.  Of the stations that serve the A train, how many are ADA compliant?

``` r
trans_entr_df |> 
    pivot_longer(
      cols = route1:route11,
      names_to = "route_num", 
      values_to = "route") |> 
    filter(route == "A", ada == TRUE) |> 
    select(station_name, line) |>
    distinct()
```

    ## # A tibble: 17 × 2
    ##    station_name                  line            
    ##    <chr>                         <chr>           
    ##  1 14th St                       8 Avenue        
    ##  2 168th St - Washington Heights 8 Avenue        
    ##  3 175th St                      8 Avenue        
    ##  4 34th St                       8 Avenue        
    ##  5 42nd St                       8 Avenue        
    ##  6 59th St                       8 Avenue        
    ##  7 Inwood - 207th St             8 Avenue        
    ##  8 West 4th St                   8 Avenue        
    ##  9 World Trade Center            8 Avenue        
    ## 10 Times Square-42nd St          Broadway        
    ## 11 59th St-Columbus Circle       Broadway-7th Ave
    ## 12 Times Square                  Broadway-7th Ave
    ## 13 8th Av                        Canarsie        
    ## 14 Franklin Av                   Franklin        
    ## 15 Euclid Av                     Fulton          
    ## 16 Franklin Av                   Fulton          
    ## 17 Howard Beach                  Rockaway

- Of the stations that serve the A train, 17 stations are ADA compliant.

# Problem 2

## Read and clean the Mr. Trash Wheel sheet

- specify the sheet in the Excel file and to omit non-data entries (rows
  with notes / figures; columns containing notes) using arguments in
  read_excel
- use reasonable variable names
- omit rows that do not include dumpster-specific data
- round the number of sports balls to the nearest integer and converts
  the result to an integer variable (using as.integer)

``` r
mr_trash_whl_df =
  read_excel("data/202409_Trash_Wheel_Collection_Data.xlsx" , 
             #specify the sheet in the Excel file
             sheet = 'Mr. Trash Wheel',
             #omit non-data entries
             range = cell_cols(1:14),
             skip = 1,
             na = c("NA", "", "."))|> 
  #use reasonable variable names
  janitor::clean_names()|> 
  #omit rows that do not include dumpster-specific data
  drop_na(dumpster) |>
  #round the number of sports balls to the nearest integer and converts the result to an integer variable
  mutate(sports_balls = as.integer(round(sports_balls)))
```

## Read and clean the Professor Trash Wheel sheet

``` r
prof_trash_whl_df = 
  read_excel("data/202409_Trash_Wheel_Collection_Data.xlsx" , 
             #specify the sheet in the Excel file
             sheet = 'Professor Trash Wheel',
             #omit non-data entries
             skip = 1, 
             na = c("NA", "", "."))|> 
  #use reasonable variable names
  janitor::clean_names()|> 
  #omit rows that do not include dumpster-specific data
  drop_na(dumpster) |>
  #round the number of sports balls to the nearest integer and converts the result to an integer variable
  mutate(trash_wheel = "Professor Trash Wheel")
```

## Read and clean the Gwynnda Trash Wheel sheet

``` r
gwynd_trash_whl_df = 
  read_excel("data/202409_Trash_Wheel_Collection_Data.xlsx" , 
             #specify the sheet in the Excel file
             sheet = 'Gwynnda Trash Wheel',
             #omit non-data entries
             skip = 1, 
             na = c("NA", "", "."))|> 
  #use reasonable variable names
  janitor::clean_names()|> 
  #omit rows that do not include dumpster-specific data
  drop_na(dumpster) |>
  #round the number of sports balls to the nearest integer and converts the result to an integer variable
  mutate(trash_wheel = "Gwynnda Trash Wheel")
```

## Combine these three datasets

- produce a single tidy dataset.
- add an additional variable to both datasets before combining to keep
  track of which Trash Wheel is.

``` r
mr_trash_whl_df = mutate(mr_trash_whl_df, 
                         year = as.double(year),
                         trash_wheel = "Mr. Trash Wheel")

sum_trash_whl_df = 
  bind_rows(mr_trash_whl_df, prof_trash_whl_df, gwynd_trash_whl_df) |> 
  pivot_longer(cols = volume_cubic_yards:sports_balls, 
               names_to = "trash_type", 
               values_to = "amount")|> 
  relocate(trash_wheel) 
```

## Write a paragraph about these data

The combined dataset contains 8264 rows, 1033 observations, providing
detailed information on trash-related information collected by three
trash wheels in Maryland. Key variables in the dataset include
trash_wheel, dumpster, month, year, date, weight_tons, homes_powered,
trash_type, amount. The total weight of trash collected by Professor
Trash Wheel is 246.74 tons. Additionally, the total number of cigarette
butts collected by Gwynnda in June of 2022 is 1.812^{4}.

# Problem 3

## Create a single, well-organized dataset with all the information contained in these data files.

### Import, clean, tidy, and wrangle of `bakers`

``` r
baker_df = read_csv('data/gbb_datasets/bakers.csv',na = c("NA", "", ".")) |>
  janitor::clean_names()|> 
  separate(baker_name, into = c("baker_fist_name", "baker_last_name"), sep = " ")
```

### Import, clean, tidy, and wrangle of \``bakes`

``` r
bake_df = read_csv('data/gbb_datasets/bakes.csv',
                   na = c("NA", "", ".") )|> 
 janitor::clean_names() |>
 rename(baker_fist_name = baker)
```

### Import, clean, tidy, and wrangle of `results`

``` r
result_df = read_csv('data/gbb_datasets/results.csv',na = c("NA", "", "."),
                     skip = 2) |>
  janitor::clean_names() |>
  rename(baker_fist_name = baker) |>
  mutate(
    result = case_match(
      result, 
      "IN" ~ "stayed in",
      "OUT" ~ "Eliminated",
      "STAR BAKER" ~ "Star Baker",
      "WINNER" ~ "Series Winner",
      "Runner-up" ~ "Series Runner up",
      "WD" ~ "withdrew"
    )
  )
```

### Check for completeness and correctness across datasets

e.g. by viewing individual datasets and using anti_join

#### 1. `bake_df` and `baker_df`

``` r
anti_join(bake_df,baker_df, by = "baker_fist_name", "series")
```

    ## # A tibble: 8 × 5
    ##   series episode baker_fist_name signature_bake                     show_stopper
    ##    <dbl>   <dbl> <chr>           <chr>                              <chr>       
    ## 1      2       1 "\"Jo\""        Chocolate Orange CupcakesOrange a… Chocolate a…
    ## 2      2       2 "\"Jo\""        Caramelised Onion, Gruyere and Th… Raspberry a…
    ## 3      2       3 "\"Jo\""        Stromboli flavored with Mozzarell… Unknown     
    ## 4      2       4 "\"Jo\""        Lavender Biscuits                  Blueberry M…
    ## 5      2       5 "\"Jo\""        Salmon and Asparagus Pie           Apple and R…
    ## 6      2       6 "\"Jo\""        Rum and Raisin Baked Cheesecake    Limoncello …
    ## 7      2       7 "\"Jo\""        Raspberry & Strawberry Mousse Cake Pain Aux Ra…
    ## 8      2       8 "\"Jo\""        Raspberry and Blueberry Mille Feu… Mini Victor…

``` r
anti_join(baker_df,bake_df, by = "baker_fist_name", "series")
```

    ## # A tibble: 23 × 6
    ##    baker_fist_name baker_last_name series baker_age baker_occupation    hometown
    ##    <chr>           <chr>            <dbl>     <dbl> <chr>               <chr>   
    ##  1 Alice           Fevronia            10        28 Geography teacher   Essex   
    ##  2 Amelia          LeBruin             10        24 Fashion designer    Halifax 
    ##  3 Antony          Amourdoux            9        30 Banker              London  
    ##  4 Briony          Williams             9        33 Full-time parent    Bristol 
    ##  5 Dan             Beasley-Harling      9        36 Full-time parent    London  
    ##  6 Dan             Chambers            10        32 Support worker      Rotherh…
    ##  7 Helena          Garcia              10        40 Online project man… Leeds   
    ##  8 Henry           Bird                10        20 Student             Durham  
    ##  9 Imelda          McCarron             9        33 Countryside recrea… County …
    ## 10 Jamie           Finn                10        20 Part-time waiter    Surrey  
    ## # ℹ 13 more rows

I find that “Jo” appearing in the `baker_fist_name column` of the
`bake_df` does not appear in the `baker_fist_name column` of `baker_df`.
There are 23 bakers informationn apearing in the `baker_df` but the
`bake_df` does not includes the information of their bakes. This is a
lack of completeness and correctness across these two datasets.

#### 2. `bake_df` and `result_df`

``` r
anti_join(bake_df, result_df, by = "baker_fist_name")
```

    ## # A tibble: 8 × 5
    ##   series episode baker_fist_name signature_bake                     show_stopper
    ##    <dbl>   <dbl> <chr>           <chr>                              <chr>       
    ## 1      2       1 "\"Jo\""        Chocolate Orange CupcakesOrange a… Chocolate a…
    ## 2      2       2 "\"Jo\""        Caramelised Onion, Gruyere and Th… Raspberry a…
    ## 3      2       3 "\"Jo\""        Stromboli flavored with Mozzarell… Unknown     
    ## 4      2       4 "\"Jo\""        Lavender Biscuits                  Blueberry M…
    ## 5      2       5 "\"Jo\""        Salmon and Asparagus Pie           Apple and R…
    ## 6      2       6 "\"Jo\""        Rum and Raisin Baked Cheesecake    Limoncello …
    ## 7      2       7 "\"Jo\""        Raspberry & Strawberry Mousse Cake Pain Aux Ra…
    ## 8      2       8 "\"Jo\""        Raspberry and Blueberry Mille Feu… Mini Victor…

``` r
anti_join(result_df, bake_df, by = "baker_fist_name")
```

    ## # A tibble: 228 × 5
    ##    series episode baker_fist_name technical result       
    ##     <dbl>   <dbl> <chr>               <dbl> <chr>        
    ##  1      2       1 Joanne                 11 stayed in    
    ##  2      2       2 Joanne                 10 stayed in    
    ##  3      2       3 Joanne                  1 stayed in    
    ##  4      2       4 Joanne                  8 stayed in    
    ##  5      2       5 Joanne                  6 stayed in    
    ##  6      2       6 Joanne                  1 Star Baker   
    ##  7      2       7 Joanne                  3 stayed in    
    ##  8      2       8 Joanne                  1 Series Winner
    ##  9      9       1 Antony                 12 stayed in    
    ## 10      9       1 Briony                  2 stayed in    
    ## # ℹ 218 more rows

I find that “Jo” appearing in the `baker_fist_name column` of the
`result_df` does not appear in the `baker_fist_name column` of `bake_df`
while Joanne appearing in the `baker_fist_name column` of the
`result`\_df`does not appear in the`baker_fist_name column`of`bake_df\`.
This is may be an typo error or somethinng wrong, which shows a lack of
completeness and correctness across these two datasets.

#### 3. `baker_df` and `result_df`

``` r
anti_join(baker_df, result_df, by = "baker_fist_name")
```

    ## # A tibble: 1 × 6
    ##   baker_fist_name baker_last_name series baker_age baker_occupation hometown    
    ##   <chr>           <chr>            <dbl>     <dbl> <chr>            <chr>       
    ## 1 Jo              Wheatley             2        41 Housewife        Ongar, Essex

``` r
anti_join(result_df,baker_df, by = "baker_fist_name")
```

    ## # A tibble: 8 × 5
    ##   series episode baker_fist_name technical result       
    ##    <dbl>   <dbl> <chr>               <dbl> <chr>        
    ## 1      2       1 Joanne                 11 stayed in    
    ## 2      2       2 Joanne                 10 stayed in    
    ## 3      2       3 Joanne                  1 stayed in    
    ## 4      2       4 Joanne                  8 stayed in    
    ## 5      2       5 Joanne                  6 stayed in    
    ## 6      2       6 Joanne                  1 Star Baker   
    ## 7      2       7 Joanne                  3 stayed in    
    ## 8      2       8 Joanne                  1 Series Winner

I find that “Jo” appearing in the `baker_fist_name column` of the
`result_df` does not appear in the `baker_fist_name column` of
`baker_df` while “Joanne” appearing in the `baker_fist_name column` of
the `result_df` does not appear in the `baker_fist_name column` of
`bake_df`. This is may be an typo error or somethinng wrong, which shows
a lack of completeness and correctness across these two datasets.

In a sum, this datasets lacks completeness and correctness across each
other.

### Merge to a single, final dataset and organize it

To make variables and observations in meaningful orders.

``` r
result_baker_bake_df = result_df |>
  left_join(baker_df, by = c("series", "baker_fist_name")) |>
  left_join(bake_df, by = c("series", "baker_fist_name", "episode")) |>
  relocate(baker_last_name, .after = baker_fist_name)|>
  relocate(technical, .before =show_stopper)
```

### Export the result file

Save as a CSV in the directory containing the original datasets.

``` r
write_csv(result_baker_bake_df, "data/result_baker_bake.csv")
```

## Describe data cleaning process

Including any questions you have or choices made. Briefly discuss the
final dataset.<br>

1.  The data cleaning process involved several key steps. First, all
    three datasets (`bakers.csv`, `bakes.csv`, and `results.csv`) were
    imported, specified the missing values and standardized column names
    to lowercase with underscores, enhancing clarity and consistency. In
    `baker_df`, the `baker_name` column was split into `baker_fist_name`
    and `baker_last_name` for better identification of individual bakers
    and facilitate the future joining with the othe two datasets. Both
    `bake_df` and `result_df` had the `baker` column renamed to
    `baker_fist_name` to maintain consisitency across datasets. Lastly,
    in `result_df`, I convert coded values of the `result` column into
    more descriptive labels, such as “IN” becoming “stayed in” and “OUT”
    changing to “Eliminated.”
2.  The question for me is “Is it the data clean enought right now? For
    example, the entry of the `hometown` variable in `bakers.csv`
    contain both information of town and area/county, sepereated by
    comma. I have thought to seperate it, but ending with not for I
    don’t think this would influence the final dataset very much and can
    do it later if necessary.
3.  The final datasets records the “Star Baker” of each episodes of each
    season, with the information of the bake and their bakes of that
    day. There are 11 variables and 1136 episodes in sum.

## Analyzing star baker trends

Create a reader-friendly table showing the star baker or winner of each
episode in Seasons 5 through 10. Comment on this table – were there any
predictable overall winners? Any surprises? **One methpod**

``` r
result_baker_bake_df |> 
  filter(series >= 5 & series <= 10, result == "Star Baker" | result == "Series Winner") |> 
  select(series, episode, baker_fist_name)|> 
  pivot_wider(names_from = series, 
          values_from = baker_fist_name , 
          names_prefix = "baker_star_winner_season_")|> 
knitr::kable()
```

| episode | baker_star_winner_season_5 | baker_star_winner_season_6 | baker_star_winner_season_7 | baker_star_winner_season_8 | baker_star_winner_season_9 | baker_star_winner_season_10 |
|--------:|:---------------------------|:---------------------------|:---------------------------|:---------------------------|:---------------------------|:----------------------------|
|       1 | Nancy                      | Marie                      | Jane                       | Steven                     | Manon                      | Michelle                    |
|       2 | Richard                    | Ian                        | Candice                    | Steven                     | Rahul                      | Alice                       |
|       3 | Luis                       | Ian                        | Tom                        | Julia                      | Rahul                      | Michael                     |
|       4 | Richard                    | Ian                        | Benjamina                  | Kate                       | Dan                        | Steph                       |
|       5 | Kate                       | Nadiya                     | Candice                    | Sophie                     | Kim-Joy                    | Steph                       |
|       6 | Chetna                     | Mat                        | Tom                        | Liam                       | Briony                     | Steph                       |
|       7 | Richard                    | Tamal                      | Andrew                     | Steven                     | Kim-Joy                    | Henry                       |
|       8 | Richard                    | Nadiya                     | Candice                    | Stacey                     | Ruby                       | Steph                       |
|       9 | Richard                    | Nadiya                     | Andrew                     | Sophie                     | Ruby                       | Alice                       |
|      10 | Nancy                      | Nadiya                     | Candice                    | Sophie                     | Rahul                      | David                       |

**Conclusion** Based on the total number of wins, the most predictable
overall winner would be Richard Burr. Interestingly, despite his 5 wins
in Season 5, he didn’t win any further times in Seasons 6 through 10. If
we focus on the top performer in Season 10, Steph Blackwell stands out
as the overall winner, as all her 4 wins were achieved during that
season.

## Viewership data in seasons 1 & 5

Import, clean, tidy, and organize the viewership data in viewers.csv.
Show the first 10 rows of this dataset.

``` r
viewer_df = read_csv('data/gbb_datasets/viewers.csv', na = c("NA", "", ".")) |> 
  janitor::clean_names() 
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
head(viewer_df, 10) |> 
knitr::kable()
```

| episode | series_1 | series_2 | series_3 | series_4 | series_5 | series_6 | series_7 | series_8 | series_9 | series_10 |
|--------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|----------:|
|       1 |     2.24 |     3.10 |     3.85 |     6.60 |    8.510 |    11.62 |    13.58 |     9.46 |     9.55 |      9.62 |
|       2 |     3.00 |     3.53 |     4.60 |     6.65 |    8.790 |    11.59 |    13.45 |     9.23 |     9.31 |      9.38 |
|       3 |     3.00 |     3.82 |     4.53 |     7.17 |    9.280 |    12.01 |    13.01 |     8.68 |     8.91 |      8.94 |
|       4 |     2.60 |     3.60 |     4.71 |     6.82 |   10.250 |    12.36 |    13.29 |     8.55 |     8.88 |      8.96 |
|       5 |     3.03 |     3.83 |     4.61 |     6.95 |    9.950 |    12.39 |    13.12 |     8.61 |     8.67 |      9.26 |
|       6 |     2.75 |     4.25 |     4.82 |     7.32 |   10.130 |    12.00 |    13.13 |     8.61 |     8.91 |      8.70 |
|       7 |       NA |     4.42 |     5.10 |     7.76 |   10.280 |    12.35 |    13.45 |     9.01 |     9.22 |      8.98 |
|       8 |       NA |     5.06 |     5.35 |     7.41 |    9.023 |    11.09 |    13.26 |     8.95 |     9.69 |      9.19 |
|       9 |       NA |       NA |     5.70 |     7.41 |   10.670 |    12.65 |    13.44 |     9.03 |     9.50 |      9.34 |
|      10 |       NA |       NA |     6.74 |     9.45 |   13.510 |    15.05 |    15.90 |    10.04 |    10.34 |     10.05 |

## Average viewership in Season 1 and 5

``` r
# Because there are null value in viewership in Season 1, we should only calculate the mean among those non-null values, which is row 1-6:
viewer_df |> 
  slice(1:6)|> 
  pull(series_1) |>
  mean()
```

    ## [1] 2.77

``` r
# Viewership in Season 5
viewer_df |> 
  pull(series_5) |>
  mean()
```

    ## [1] 10.0393

The average viewership in Season 1 is 2.77. The average viewership in
Season 5 is 10.0393.
