HW2 Data Wrangling I
================
Minghui Wang
2024-10-01

[HW2](https://p8105.com/homework_2.html) assignment reinforces ideas in
[Data Wrangling I](https://p8105.com/topic_data_wrangling_i.html). Here
are the codes focusing on question 1-3.

# Problem 1

## Import and clean the NYC Transit.csv

``` r
trans_entr_df = 
  read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
  col_types = cols( "Route8" = "c","Route9" = "c","Route10" = "c","Route11" = "c")) |>
  janitor::clean_names() |> 
  select(
    line, station_name, station_latitude, station_longitude, starts_with("route"), entry, vending, entrance_type, ada
  ) |> 
  mutate(entry = if_else(entry == "YES", TRUE, FALSE))
```

**A short paragraph about this dataset**: This dataset contains 32
variables, with 22 being character variables (such as Division, Line,
Station Name, and Route1 to Route7), 8 being numeric variables
(including Station Latitude, Station Longitude, Entrance Latitude, and
Entrance Longitude), and 2 being logical variables (ADA and Free
Crossover). So far, I have standardized the column names by converting
them to lowercase and treated all route variables as character types.
The resulting dataset has dimensions of 1868 rows and 19 columns.
Currently, the data is not tidy because the route numbers are spread
across multiple columns (from “Route1” to “Route11”) rather than being
combined into a single column as a variable.

## Answer Questions

1.  How many distinct stations are there? Note that stations are
    identified both by name and by line (e.g. 125th St 8th Avenue; 125st
    Broadway; 125st Lenox); the distinct function may be useful here.

``` r
trans_entr_df |> 
  select(station_name, line) |>
  distinct()
```

    ## # A tibble: 465 × 2
    ##    station_name             line    
    ##    <chr>                    <chr>   
    ##  1 25th St                  4 Avenue
    ##  2 36th St                  4 Avenue
    ##  3 45th St                  4 Avenue
    ##  4 53rd St                  4 Avenue
    ##  5 59th St                  4 Avenue
    ##  6 77th St                  4 Avenue
    ##  7 86th St                  4 Avenue
    ##  8 95th St                  4 Avenue
    ##  9 9th St                   4 Avenue
    ## 10 Atlantic Av-Barclays Ctr 4 Avenue
    ## # ℹ 455 more rows

- There are 465 distinct stations.

2.  How many stations are ADA compliant?

``` r
trans_entr_df |> 
  filter(ada == "TRUE")|>
  select(station_name, line) |>
  distinct()
```

    ## # A tibble: 84 × 2
    ##    station_name                   line           
    ##    <chr>                          <chr>          
    ##  1 Atlantic Av-Barclays Ctr       4 Avenue       
    ##  2 DeKalb Av                      4 Avenue       
    ##  3 Pacific St                     4 Avenue       
    ##  4 Grand Central                  42nd St Shuttle
    ##  5 34th St                        6 Avenue       
    ##  6 47-50th Sts Rockefeller Center 6 Avenue       
    ##  7 Church Av                      6 Avenue       
    ##  8 21st St                        63rd Street    
    ##  9 Lexington Av                   63rd Street    
    ## 10 Roosevelt Island               63rd Street    
    ## # ℹ 74 more rows

- There are 84 distinct stations which are ADA compliant.

3.  What proportion of station entrances / exits without vending allow
    entrance?

``` r
trans_entr_df |> 
  filter(vending == "NO")|>
  pull(entry) |>
  mean()
```

    ## [1] 0.3770492

- 37.70% of station entrances / exits without vending allow entrance.

## Reformat data

To make route number and route name distinct variables. 1. How many
distinct stations serve the A train?

``` r
trans_entr_df |> 
    pivot_longer(
      cols = route1:route11,
      names_to = "route_num", 
      values_to = "route") |> 
    filter(route == "A") |> 
    select(station_name, line) |>
    distinct()
```

- There are 60 distinct stations serve the A train.

2.  Of the stations that serve the A train, how many are ADA compliant?

``` r
trans_entr_df |> 
    pivot_longer(
      cols = route1:route11,
      names_to = "route_num", 
      values_to = "route") |> 
    filter(route == "A", ada == TRUE) |> 
    select(station_name, line) |>
    distinct()
```

    ## # A tibble: 17 × 2
    ##    station_name                  line            
    ##    <chr>                         <chr>           
    ##  1 14th St                       8 Avenue        
    ##  2 168th St - Washington Heights 8 Avenue        
    ##  3 175th St                      8 Avenue        
    ##  4 34th St                       8 Avenue        
    ##  5 42nd St                       8 Avenue        
    ##  6 59th St                       8 Avenue        
    ##  7 Inwood - 207th St             8 Avenue        
    ##  8 West 4th St                   8 Avenue        
    ##  9 World Trade Center            8 Avenue        
    ## 10 Times Square-42nd St          Broadway        
    ## 11 59th St-Columbus Circle       Broadway-7th Ave
    ## 12 Times Square                  Broadway-7th Ave
    ## 13 8th Av                        Canarsie        
    ## 14 Franklin Av                   Franklin        
    ## 15 Euclid Av                     Fulton          
    ## 16 Franklin Av                   Fulton          
    ## 17 Howard Beach                  Rockaway

- Of the stations that serve the A train, 17 stations are ADA compliant.

# Problem 2

## Read and clean the Mr. Trash Wheel sheet

- specify the sheet in the Excel file and to omit non-data entries (rows
  with notes / figures; columns containing notes) using arguments in
  read_excel
- use reasonable variable names
- omit rows that do not include dumpster-specific data
- round the number of sports balls to the nearest integer and converts
  the result to an integer variable (using as.integer)

``` r
mr_trash_whl_df =
  read_excel("data/202409_Trash_Wheel_Collection_Data.xlsx" , 
             #specify the sheet in the Excel file
             sheet = 'Mr. Trash Wheel',
             #omit non-data entries
             range = cell_cols(1:14),
             skip = 1,
             na = c("NA", "", "."))|> 
  #use reasonable variable names
  janitor::clean_names()|> 
  #omit rows that do not include dumpster-specific data
  drop_na(dumpster) |>
  #round the number of sports balls to the nearest integer and converts the result to an integer variable
  mutate(sports_balls = as.integer(round(sports_balls)))
```

## Read and clean the Professor Trash Wheel sheet

``` r
prof_trash_whl_df = 
  read_excel("data/202409_Trash_Wheel_Collection_Data.xlsx" , 
             #specify the sheet in the Excel file
             sheet = 'Professor Trash Wheel',
             #omit non-data entries
             skip = 1, 
             na = c("NA", "", "."))|> 
  #use reasonable variable names
  janitor::clean_names()|> 
  #omit rows that do not include dumpster-specific data
  drop_na(dumpster) |>
  #round the number of sports balls to the nearest integer and converts the result to an integer variable
  mutate(trash_wheel = "Professor Trash Wheel")
```

## Read and clean the Gwynnda Trash Wheel sheet

``` r
gwynd_trash_whl_df = 
  read_excel("data/202409_Trash_Wheel_Collection_Data.xlsx" , 
             #specify the sheet in the Excel file
             sheet = 'Gwynnda Trash Wheel',
             #omit non-data entries
             skip = 1, 
             na = c("NA", "", "."))|> 
  #use reasonable variable names
  janitor::clean_names()|> 
  #omit rows that do not include dumpster-specific data
  drop_na(dumpster) |>
  #round the number of sports balls to the nearest integer and converts the result to an integer variable
  mutate(trash_wheel = "Gwynnda Trash Wheel")
```

## Combine these three datasets

- produce a single tidy dataset.
- add an additional variable to both datasets before combining to keep
  track of which Trash Wheel is.

``` r
mr_trash_whl_df = mutate(mr_trash_whl_df, 
                         year = as.double(year),
                         trash_wheel = "Mr. Trash Wheel")

sum_trash_whl_df = 
  bind_rows(mr_trash_whl_df, prof_trash_whl_df, gwynd_trash_whl_df) |> 
  pivot_longer(cols = volume_cubic_yards:sports_balls, 
               names_to = "trash_type", 
               values_to = "amount")|> 
  relocate(trash_wheel) 
```

## Write a paragraph about these data

The combined dataset contains 8264 observations, providing detailed
information on trash-related information collected by three trash wheels
inn Maryland. Key variables in the dataset include trash_wheel,
dumpster, month, year, date, weight_tons, homes_powered, trash_type,
amount. The total weight of trash collected by Professor Trash Wheel is
246.74 tons. Additionally, the total number of cigarette butts collected
by Gwynnda in June of 2022 is 1.812^{4}.

# Problem 3

## Create a single, well-organized dataset with all the information contained in these data files.

### Import, clean, tidy, and wrangle of bakers;

``` r
baker_df = read_csv('data/gbb_datasets/bakers.csv',na = c("NA", "", ".")) |>
  janitor::clean_names()|> 
  separate(baker_name, into = c("baker_fist_name", "baker_last_name"), sep = " ")
```

### Import, clean, tidy, and wrangle of bakes;

``` r
bake_df = read_csv('data/gbb_datasets/bakes.csv',
                   na = c("NA", "", ".") )|> 
 janitor::clean_names() |>
 rename(baker_fist_name = baker)
```

### Import, clean, tidy, and wrangle of results;

``` r
result_df = read_csv('data/gbb_datasets/results.csv',na = c("NA", "", "."),
                     skip = 2) |>
  janitor::clean_names() |>
  rename(baker_fist_name = baker) |>
  mutate(
    result = case_match(
      result, 
      "IN" ~ "stayed in",
      "OUT" ~ "Eliminated",
      "STAR BAKER" ~ "Star Baker",
      "WINNER" ~ "Series Winner",
      "Runner-up" ~ "Series Runner up",
      "WD" ~ "withdrew"
    )
  )
```

### Check for completeness and correctness across datasets

e.g. by viewing individual datasets and using anti_join

``` r
anti_join(baker_df, bake_df)
```

    ## Joining with `by = join_by(baker_fist_name, series)`

    ## # A tibble: 26 × 6
    ##    baker_fist_name baker_last_name series baker_age baker_occupation    hometown
    ##    <chr>           <chr>            <dbl>     <dbl> <chr>               <chr>   
    ##  1 Alice           Fevronia            10        28 Geography teacher   Essex   
    ##  2 Amelia          LeBruin             10        24 Fashion designer    Halifax 
    ##  3 Antony          Amourdoux            9        30 Banker              London  
    ##  4 Briony          Williams             9        33 Full-time parent    Bristol 
    ##  5 Dan             Beasley-Harling      9        36 Full-time parent    London  
    ##  6 Dan             Chambers            10        32 Support worker      Rotherh…
    ##  7 David           Atherton            10        36 International heal… Whitby  
    ##  8 Helena          Garcia              10        40 Online project man… Leeds   
    ##  9 Henry           Bird                10        20 Student             Durham  
    ## 10 Imelda          McCarron             9        33 Countryside recrea… County …
    ## # ℹ 16 more rows

``` r
anti_join(result_df, baker_df)
```

    ## Joining with `by = join_by(series, baker_fist_name)`

    ## # A tibble: 8 × 5
    ##   series episode baker_fist_name technical result       
    ##    <dbl>   <dbl> <chr>               <dbl> <chr>        
    ## 1      2       1 Joanne                 11 stayed in    
    ## 2      2       2 Joanne                 10 stayed in    
    ## 3      2       3 Joanne                  1 stayed in    
    ## 4      2       4 Joanne                  8 stayed in    
    ## 5      2       5 Joanne                  6 stayed in    
    ## 6      2       6 Joanne                  1 Star Baker   
    ## 7      2       7 Joanne                  3 stayed in    
    ## 8      2       8 Joanne                  1 Series Winner

``` r
anti_join(result_df, bake_df)
```

    ## Joining with `by = join_by(series, episode, baker_fist_name)`

    ## # A tibble: 596 × 5
    ##    series episode baker_fist_name technical result
    ##     <dbl>   <dbl> <chr>               <dbl> <chr> 
    ##  1      1       2 Lea                    NA <NA>  
    ##  2      1       2 Mark                   NA <NA>  
    ##  3      1       3 Annetha                NA <NA>  
    ##  4      1       3 Lea                    NA <NA>  
    ##  5      1       3 Louise                 NA <NA>  
    ##  6      1       3 Mark                   NA <NA>  
    ##  7      1       4 Annetha                NA <NA>  
    ##  8      1       4 Jonathan               NA <NA>  
    ##  9      1       4 Lea                    NA <NA>  
    ## 10      1       4 Louise                 NA <NA>  
    ## # ℹ 586 more rows

### Merge to a single, final dataset and organize it

To make variables and observations in meaningful orders.

``` r
result_baker_bake_df = result_df |>
  left_join(baker_df, by = c("series", "baker_fist_name")) |>
  left_join(bake_df, by = c("series", "baker_fist_name", "episode")) |>
  relocate(baker_last_name, .after = baker_fist_name)|>
  relocate(technical, .before =show_stopper)
view(result_baker_bake_df)
```

### Export the result file

Save as a CSV in the directory containing the original datasets.

``` r
write_csv(result_baker_bake_df, "data/result_baker_bake.csv")
```

## Describe data cleaning process

including any questions you have or choices you made. Briefly discuss
the final dataset.

## Analyzing Star Baker Trends: A Look at Winners from Seasons 5 to 10

Create a reader-friendly table showing the star baker or winner of each
episode in Seasons 5 through 10. Comment on this table – were there any
predictable overall winners? Any surprises?

## Viewership Data in Seasons 1 & 5

Import, clean, tidy, and organize the viewership data in viewers.csv.
Show the first 10 rows of this dataset. What was the average viewership
in Season 1? In Season 5?
